{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from proj1_helpers import load_csv_data, create_csv_submission\n",
    "from feature_selection import select_best_degrees, build_poly_by_feature\n",
    "from feature_selection import compute_log, compute_theta, compute_physics\n",
    "from correction_rate import cross_validation, print_score\n",
    "from evaluation import predict_regression_labels\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "loading the training dataset...\n",
      "data loaded...\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## Training\n",
    "############################\n",
    "print('training started')\n",
    "\n",
    "dat_dir = '../data/'\n",
    "# load the training set\n",
    "print('loading the training dataset...')\n",
    "y_train_pre, tx_train, ids_train = load_csv_data(dat_dir + \"train.csv\", sub_sample=False)\n",
    "print('data loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the featues using log()\n",
    "index_log = [0,1,2,4,5,6,7,9,10,12,16,21,23,24,25,26,27,28,29]\n",
    "tx_log,mean_log,std_log = compute_log(tx_train, index_log)\n",
    "\n",
    "# construct the featues using cosine()\n",
    "index_theta = [14,15,17,18,20]\n",
    "tx_theta,mean_theta,std_theta = compute_theta(tx_train,index_theta)\n",
    "\n",
    "# construct the featues with physics meanings: \n",
    "# index_physics_A (mass) * index_physics_B / index_physics_C\n",
    "index_physics_A = [0]\n",
    "index_physics_B = [10,13,13,9]\n",
    "index_physics_C = [9,16,21,10]\n",
    "tx_physics, mean_physics, std_physics = compute_physics(tx_train,\n",
    "                                                        index_physics_A,\n",
    "                                                        index_physics_B,\n",
    "                                                        index_physics_C)\n",
    "\n",
    "# combine all the selected features for training set\n",
    "train_new = np.c_[tx_log, tx_theta, tx_physics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degrees = np.load('best_degrees.npy')\n",
    "# reconstruct all the features at their own best degrees\n",
    "train_best_degrees = build_poly_by_feature(train_new, best_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train_pre.reshape(y_train_pre.shape[0],1)\n",
    "# innitialize hyper parameters\n",
    "k_fold = 15\n",
    "method = 'RR'\n",
    "X = train_best_degrees\n",
    "y = np.copy(y_train)\n",
    "\n",
    "initial_w_pre = np.zeros((np.size(X,1)))\n",
    "initial_w=initial_w_pre.reshape(initial_w_pre.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "                       Least squares                        \n",
      "============================================================\n",
      "average loss         ...............................1.091e+00\n",
      "average accuracy     ...............................8.219e-01\n",
      "max accuracy         ...............................8.270e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Least squares\n",
    "scores, weights,loss_avg , score_avg = cross_validation(y, X_sigmoid, 'LS', {},\n",
    "                                                        threshold = 0,\n",
    "                                                         k=k_fold, seed=0)\n",
    "print_score(loss_avg, score_avg, scores, method='Least squares')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "                      Ridge regression                      \n",
      "============================================================\n",
      "average loss         ...............................1.092e+00\n",
      "average accuracy     ...............................8.222e-01\n",
      "max accuracy         ...............................8.280e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression\n",
    "lambda_ = 1e-7\n",
    "scores, weights, loss_avg, score_avg = cross_validation(y, X_sigmoid, 'RR', {\"lambda_\": lambda_}, \n",
    "                                                        threshold = 0, k=k_fold, seed=0)\n",
    "print_score(loss_avg, score_avg, scores, method='Ridge regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient decend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import sigmoid\n",
    "X_sigmoid = np.copy(X)\n",
    "X_sigmoid = sigmoid(X_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "gradient descent                  0/99     4.584e-01 \n",
      "gradient descent                 10/99     4.351e-01 \n",
      "gradient descent                 20/99     4.266e-01 \n",
      "gradient descent                 30/99     4.197e-01 \n",
      "gradient descent                 40/99     4.139e-01 \n",
      "gradient descent                 50/99     4.089e-01 \n",
      "gradient descent                 60/99     4.045e-01 \n",
      "gradient descent                 70/99     4.006e-01 \n",
      "gradient descent                 80/99     3.971e-01 \n",
      "gradient descent                 90/99     3.939e-01 \n",
      "gradient descent                 99/99     3.913e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "gradient descent                  0/99     4.582e-01 \n",
      "gradient descent                 10/99     4.348e-01 \n",
      "gradient descent                 20/99     4.262e-01 \n",
      "gradient descent                 30/99     4.192e-01 \n",
      "gradient descent                 40/99     4.133e-01 \n",
      "gradient descent                 50/99     4.082e-01 \n",
      "gradient descent                 60/99     4.038e-01 \n",
      "gradient descent                 70/99     3.999e-01 \n",
      "gradient descent                 80/99     3.963e-01 \n",
      "gradient descent                 90/99     3.931e-01 \n",
      "gradient descent                 99/99     3.905e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "gradient descent                  0/99     4.590e-01 \n",
      "gradient descent                 10/99     4.359e-01 \n",
      "gradient descent                 20/99     4.274e-01 \n",
      "gradient descent                 30/99     4.204e-01 \n",
      "gradient descent                 40/99     4.146e-01 \n",
      "gradient descent                 50/99     4.096e-01 \n",
      "gradient descent                 60/99     4.052e-01 \n",
      "gradient descent                 70/99     4.013e-01 \n",
      "gradient descent                 80/99     3.978e-01 \n",
      "gradient descent                 90/99     3.946e-01 \n",
      "gradient descent                 99/99     3.920e-01 \n",
      "============================================================\n",
      "                      Gradient decend                       \n",
      "============================================================\n",
      "average loss         ...............................1.280e+00\n",
      "average accuracy     ...............................6.784e-01\n",
      "max accuracy         ...............................6.809e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Gradient decend\n",
    "max_iters = 100\n",
    "gamma = 1e-2\n",
    "scores, weights,loss_avg , score_avg = cross_validation(y,X_sigmoid, 'GD', \n",
    "                                                        {\n",
    "                                                            \"initial_w\": initial_w,\n",
    "                                                            \"gamma\": gamma,\n",
    "                                                            \"max_iters\": max_iters,\n",
    "                                                        },\n",
    "                                                        threshold = 0,\n",
    "                                                         k=3, seed=0)\n",
    "\n",
    "print_score(loss_avg, score_avg, scores, method='Gradient decend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient decend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "stochastic gradient descent       0/99     4.985e-01 \n",
      "stochastic gradient descent      10/99     5.051e-01 \n",
      "stochastic gradient descent      20/99     4.949e-01 \n",
      "stochastic gradient descent      30/99     4.945e-01 \n",
      "stochastic gradient descent      40/99     4.919e-01 \n",
      "stochastic gradient descent      50/99     4.893e-01 \n",
      "stochastic gradient descent      60/99     4.866e-01 \n",
      "stochastic gradient descent      70/99     4.772e-01 \n",
      "stochastic gradient descent      80/99     4.734e-01 \n",
      "stochastic gradient descent      90/99     4.701e-01 \n",
      "stochastic gradient descent      99/99     4.695e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "stochastic gradient descent       0/99     4.985e-01 \n",
      "stochastic gradient descent      10/99     4.952e-01 \n",
      "stochastic gradient descent      20/99     4.978e-01 \n",
      "stochastic gradient descent      30/99     4.915e-01 \n",
      "stochastic gradient descent      40/99     4.804e-01 \n",
      "stochastic gradient descent      50/99     4.806e-01 \n",
      "stochastic gradient descent      60/99     4.698e-01 \n",
      "stochastic gradient descent      70/99     4.651e-01 \n",
      "stochastic gradient descent      80/99     4.606e-01 \n",
      "stochastic gradient descent      90/99     4.583e-01 \n",
      "stochastic gradient descent      99/99     4.532e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "stochastic gradient descent       0/99     4.984e-01 \n",
      "stochastic gradient descent      10/99     5.049e-01 \n",
      "stochastic gradient descent      20/99     4.946e-01 \n",
      "stochastic gradient descent      30/99     4.911e-01 \n",
      "stochastic gradient descent      40/99     4.861e-01 \n",
      "stochastic gradient descent      50/99     4.838e-01 \n",
      "stochastic gradient descent      60/99     4.844e-01 \n",
      "stochastic gradient descent      70/99     4.773e-01 \n",
      "stochastic gradient descent      80/99     4.689e-01 \n",
      "stochastic gradient descent      90/99     4.672e-01 \n",
      "stochastic gradient descent      99/99     4.646e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "stochastic gradient descent       0/99     4.983e-01 \n",
      "stochastic gradient descent      10/99     4.920e-01 \n",
      "stochastic gradient descent      20/99     4.887e-01 \n",
      "stochastic gradient descent      30/99     4.861e-01 \n",
      "stochastic gradient descent      40/99     4.759e-01 \n",
      "stochastic gradient descent      50/99     4.814e-01 \n",
      "stochastic gradient descent      60/99     4.818e-01 \n",
      "stochastic gradient descent      70/99     4.798e-01 \n",
      "stochastic gradient descent      80/99     4.730e-01 \n",
      "stochastic gradient descent      90/99     4.674e-01 \n",
      "stochastic gradient descent      99/99     4.655e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "stochastic gradient descent       0/99     4.985e-01 \n",
      "stochastic gradient descent      10/99     4.895e-01 \n",
      "stochastic gradient descent      20/99     4.893e-01 \n",
      "stochastic gradient descent      30/99     4.788e-01 \n",
      "stochastic gradient descent      40/99     4.790e-01 \n",
      "stochastic gradient descent      50/99     4.747e-01 \n",
      "stochastic gradient descent      60/99     4.803e-01 \n",
      "stochastic gradient descent      70/99     4.738e-01 \n",
      "stochastic gradient descent      80/99     4.767e-01 \n",
      "stochastic gradient descent      90/99     4.752e-01 \n",
      "stochastic gradient descent      99/99     4.774e-01 \n",
      "============================================================\n",
      "                      Gradient decend                       \n",
      "============================================================\n",
      "average loss         ...............................1.383e+00\n",
      "average accuracy     ...............................6.573e-01\n",
      "max accuracy         ...............................6.615e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient decend\n",
    "max_iters = 100\n",
    "gamma = 1e-4\n",
    "scores, weights,loss_avg , score_avg = cross_validation(y, X_sigmoid, 'SGD', \n",
    "                                                        {\n",
    "                                                            \"initial_w\": initial_w,\n",
    "                                                            \"gamma\": gamma,\n",
    "                                                            \"max_iters\": max_iters,\n",
    "                                                        },\n",
    "                                                        threshold = 0,\n",
    "                                                         k=5, seed=0)\n",
    "print_score(loss_avg, score_avg, scores, method='Gradient decend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "logistic regression               0/799    6.491e-01  -1.449e-02 -1.144e-02\n",
      "logistic regression              10/799    6.077e-01  -5.405e-03 -1.973e-02\n",
      "logistic regression              20/799    5.909e-01  -2.400e-03 -2.751e-02\n",
      "logistic regression              30/799    5.787e-01  1.176e-04  -3.262e-02\n",
      "logistic regression              40/799    5.690e-01  2.314e-03  -3.583e-02\n",
      "logistic regression              50/799    5.611e-01  4.280e-03  -3.764e-02\n",
      "logistic regression              60/799    5.543e-01  6.072e-03  -3.837e-02\n",
      "logistic regression              70/799    5.486e-01  7.727e-03  -3.825e-02\n",
      "logistic regression              80/799    5.436e-01  9.271e-03  -3.745e-02\n",
      "logistic regression              90/799    5.393e-01  1.072e-02  -3.610e-02\n",
      "logistic regression             100/799    5.354e-01  1.210e-02  -3.430e-02\n",
      "logistic regression             110/799    5.320e-01  1.342e-02  -3.213e-02\n",
      "logistic regression             120/799    5.290e-01  1.468e-02  -2.964e-02\n",
      "logistic regression             130/799    5.262e-01  1.589e-02  -2.690e-02\n",
      "logistic regression             140/799    5.236e-01  1.706e-02  -2.394e-02\n",
      "logistic regression             150/799    5.213e-01  1.820e-02  -2.080e-02\n",
      "logistic regression             160/799    5.192e-01  1.931e-02  -1.751e-02\n",
      "logistic regression             170/799    5.172e-01  2.038e-02  -1.408e-02\n",
      "logistic regression             180/799    5.154e-01  2.144e-02  -1.055e-02\n",
      "logistic regression             190/799    5.137e-01  2.248e-02  -6.927e-03\n",
      "logistic regression             200/799    5.121e-01  2.349e-02  -3.226e-03\n",
      "logistic regression             210/799    5.106e-01  2.449e-02  5.394e-04 \n",
      "logistic regression             220/799    5.092e-01  2.547e-02  4.358e-03 \n",
      "logistic regression             230/799    5.079e-01  2.645e-02  8.220e-03 \n",
      "logistic regression             240/799    5.066e-01  2.740e-02  1.212e-02 \n",
      "logistic regression             250/799    5.055e-01  2.835e-02  1.605e-02 \n",
      "logistic regression             260/799    5.043e-01  2.929e-02  2.000e-02 \n",
      "logistic regression             270/799    5.032e-01  3.021e-02  2.396e-02 \n",
      "logistic regression             280/799    5.022e-01  3.113e-02  2.794e-02 \n",
      "logistic regression             290/799    5.012e-01  3.204e-02  3.193e-02 \n",
      "logistic regression             300/799    5.003e-01  3.294e-02  3.593e-02 \n",
      "logistic regression             310/799    4.993e-01  3.384e-02  3.992e-02 \n",
      "logistic regression             320/799    4.985e-01  3.472e-02  4.392e-02 \n",
      "logistic regression             330/799    4.976e-01  3.561e-02  4.791e-02 \n",
      "logistic regression             340/799    4.968e-01  3.648e-02  5.190e-02 \n",
      "logistic regression             350/799    4.960e-01  3.735e-02  5.588e-02 \n",
      "logistic regression             360/799    4.953e-01  3.822e-02  5.985e-02 \n",
      "logistic regression             370/799    4.945e-01  3.908e-02  6.381e-02 \n",
      "logistic regression             380/799    4.938e-01  3.993e-02  6.776e-02 \n",
      "logistic regression             390/799    4.931e-01  4.078e-02  7.170e-02 \n",
      "logistic regression             400/799    4.925e-01  4.163e-02  7.562e-02 \n",
      "logistic regression             410/799    4.918e-01  4.247e-02  7.953e-02 \n",
      "logistic regression             420/799    4.912e-01  4.330e-02  8.343e-02 \n",
      "logistic regression             430/799    4.906e-01  4.414e-02  8.731e-02 \n",
      "logistic regression             440/799    4.900e-01  4.497e-02  9.118e-02 \n",
      "logistic regression             450/799    4.894e-01  4.579e-02  9.503e-02 \n",
      "logistic regression             460/799    4.888e-01  4.661e-02  9.886e-02 \n",
      "logistic regression             470/799    4.883e-01  4.743e-02  1.027e-01 \n",
      "logistic regression             480/799    4.877e-01  4.824e-02  1.065e-01 \n",
      "logistic regression             490/799    4.872e-01  4.905e-02  1.103e-01 \n",
      "logistic regression             500/799    4.867e-01  4.985e-02  1.140e-01 \n",
      "logistic regression             510/799    4.862e-01  5.065e-02  1.178e-01 \n",
      "logistic regression             520/799    4.857e-01  5.145e-02  1.215e-01 \n",
      "logistic regression             530/799    4.853e-01  5.224e-02  1.252e-01 \n",
      "logistic regression             540/799    4.848e-01  5.303e-02  1.289e-01 \n",
      "logistic regression             550/799    4.843e-01  5.382e-02  1.326e-01 \n",
      "logistic regression             560/799    4.839e-01  5.460e-02  1.363e-01 \n",
      "logistic regression             570/799    4.835e-01  5.538e-02  1.399e-01 \n",
      "logistic regression             580/799    4.830e-01  5.616e-02  1.435e-01 \n",
      "logistic regression             590/799    4.826e-01  5.693e-02  1.471e-01 \n",
      "logistic regression             600/799    4.822e-01  5.770e-02  1.507e-01 \n",
      "logistic regression             610/799    4.818e-01  5.846e-02  1.543e-01 \n",
      "logistic regression             620/799    4.814e-01  5.922e-02  1.579e-01 \n",
      "logistic regression             630/799    4.810e-01  5.998e-02  1.614e-01 \n",
      "logistic regression             640/799    4.807e-01  6.073e-02  1.650e-01 \n",
      "logistic regression             650/799    4.803e-01  6.148e-02  1.685e-01 \n",
      "logistic regression             660/799    4.799e-01  6.223e-02  1.720e-01 \n",
      "logistic regression             670/799    4.796e-01  6.297e-02  1.755e-01 \n",
      "logistic regression             680/799    4.792e-01  6.371e-02  1.789e-01 \n",
      "logistic regression             690/799    4.789e-01  6.445e-02  1.824e-01 \n",
      "logistic regression             700/799    4.785e-01  6.518e-02  1.858e-01 \n",
      "logistic regression             710/799    4.782e-01  6.591e-02  1.892e-01 \n",
      "logistic regression             720/799    4.779e-01  6.664e-02  1.927e-01 \n",
      "logistic regression             730/799    4.775e-01  6.736e-02  1.961e-01 \n",
      "logistic regression             740/799    4.772e-01  6.808e-02  1.994e-01 \n",
      "logistic regression             750/799    4.769e-01  6.879e-02  2.028e-01 \n",
      "logistic regression             760/799    4.766e-01  6.950e-02  2.062e-01 \n",
      "logistic regression             770/799    4.763e-01  7.021e-02  2.095e-01 \n",
      "logistic regression             780/799    4.760e-01  7.092e-02  2.128e-01 \n",
      "logistic regression             790/799    4.757e-01  7.162e-02  2.161e-01 \n",
      "logistic regression             799/799    4.754e-01  7.225e-02  2.191e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "logistic regression               0/799    6.504e-01  -1.427e-02 -1.129e-02\n",
      "logistic regression              10/799    6.093e-01  -5.329e-03 -1.954e-02\n",
      "logistic regression              20/799    5.927e-01  -2.399e-03 -2.717e-02\n",
      "logistic regression              30/799    5.806e-01  4.545e-05  -3.211e-02\n",
      "logistic regression              40/799    5.711e-01  2.173e-03  -3.515e-02\n",
      "logistic regression              50/799    5.632e-01  4.073e-03  -3.677e-02\n",
      "logistic regression              60/799    5.566e-01  5.801e-03  -3.733e-02\n",
      "logistic regression              70/799    5.509e-01  7.395e-03  -3.703e-02\n",
      "logistic regression              80/799    5.460e-01  8.880e-03  -3.607e-02\n",
      "logistic regression              90/799    5.417e-01  1.028e-02  -3.455e-02\n",
      "logistic regression             100/799    5.379e-01  1.160e-02  -3.259e-02\n",
      "logistic regression             110/799    5.345e-01  1.286e-02  -3.026e-02\n",
      "logistic regression             120/799    5.314e-01  1.407e-02  -2.762e-02\n",
      "logistic regression             130/799    5.286e-01  1.523e-02  -2.474e-02\n",
      "logistic regression             140/799    5.261e-01  1.635e-02  -2.164e-02\n",
      "logistic regression             150/799    5.238e-01  1.744e-02  -1.836e-02\n",
      "logistic regression             160/799    5.217e-01  1.850e-02  -1.493e-02\n",
      "logistic regression             170/799    5.197e-01  1.954e-02  -1.138e-02\n",
      "logistic regression             180/799    5.179e-01  2.055e-02  -7.730e-03\n",
      "logistic regression             190/799    5.162e-01  2.154e-02  -3.990e-03\n",
      "logistic regression             200/799    5.146e-01  2.252e-02  -1.781e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression             210/799    5.131e-01  2.348e-02  3.693e-03 \n",
      "logistic regression             220/799    5.117e-01  2.442e-02  7.612e-03 \n",
      "logistic regression             230/799    5.103e-01  2.536e-02  1.157e-02 \n",
      "logistic regression             240/799    5.090e-01  2.628e-02  1.556e-02 \n",
      "logistic regression             250/799    5.078e-01  2.719e-02  1.957e-02 \n",
      "logistic regression             260/799    5.067e-01  2.809e-02  2.361e-02 \n",
      "logistic regression             270/799    5.056e-01  2.898e-02  2.765e-02 \n",
      "logistic regression             280/799    5.045e-01  2.987e-02  3.170e-02 \n",
      "logistic regression             290/799    5.035e-01  3.074e-02  3.576e-02 \n",
      "logistic regression             300/799    5.026e-01  3.161e-02  3.982e-02 \n",
      "logistic regression             310/799    5.016e-01  3.248e-02  4.388e-02 \n",
      "logistic regression             320/799    5.007e-01  3.334e-02  4.793e-02 \n",
      "logistic regression             330/799    4.999e-01  3.419e-02  5.198e-02 \n",
      "logistic regression             340/799    4.991e-01  3.504e-02  5.601e-02 \n",
      "logistic regression             350/799    4.983e-01  3.588e-02  6.004e-02 \n",
      "logistic regression             360/799    4.975e-01  3.672e-02  6.405e-02 \n",
      "logistic regression             370/799    4.967e-01  3.755e-02  6.806e-02 \n",
      "logistic regression             380/799    4.960e-01  3.838e-02  7.204e-02 \n",
      "logistic regression             390/799    4.953e-01  3.921e-02  7.602e-02 \n",
      "logistic regression             400/799    4.946e-01  4.003e-02  7.997e-02 \n",
      "logistic regression             410/799    4.940e-01  4.085e-02  8.391e-02 \n",
      "logistic regression             420/799    4.933e-01  4.166e-02  8.783e-02 \n",
      "logistic regression             430/799    4.927e-01  4.247e-02  9.174e-02 \n",
      "logistic regression             440/799    4.921e-01  4.328e-02  9.563e-02 \n",
      "logistic regression             450/799    4.915e-01  4.408e-02  9.949e-02 \n",
      "logistic regression             460/799    4.909e-01  4.488e-02  1.033e-01 \n",
      "logistic regression             470/799    4.904e-01  4.568e-02  1.072e-01 \n",
      "logistic regression             480/799    4.898e-01  4.647e-02  1.110e-01 \n",
      "logistic regression             490/799    4.893e-01  4.726e-02  1.148e-01 \n",
      "logistic regression             500/799    4.887e-01  4.804e-02  1.186e-01 \n",
      "logistic regression             510/799    4.882e-01  4.883e-02  1.223e-01 \n",
      "logistic regression             520/799    4.877e-01  4.961e-02  1.261e-01 \n",
      "logistic regression             530/799    4.873e-01  5.038e-02  1.298e-01 \n",
      "logistic regression             540/799    4.868e-01  5.115e-02  1.335e-01 \n",
      "logistic regression             550/799    4.863e-01  5.192e-02  1.372e-01 \n",
      "logistic regression             560/799    4.859e-01  5.269e-02  1.408e-01 \n",
      "logistic regression             570/799    4.854e-01  5.345e-02  1.445e-01 \n",
      "logistic regression             580/799    4.850e-01  5.421e-02  1.481e-01 \n",
      "logistic regression             590/799    4.846e-01  5.496e-02  1.517e-01 \n",
      "logistic regression             600/799    4.841e-01  5.572e-02  1.553e-01 \n",
      "logistic regression             610/799    4.837e-01  5.647e-02  1.589e-01 \n",
      "logistic regression             620/799    4.833e-01  5.721e-02  1.624e-01 \n",
      "logistic regression             630/799    4.829e-01  5.795e-02  1.660e-01 \n",
      "logistic regression             640/799    4.825e-01  5.869e-02  1.695e-01 \n",
      "logistic regression             650/799    4.822e-01  5.943e-02  1.730e-01 \n",
      "logistic regression             660/799    4.818e-01  6.016e-02  1.765e-01 \n",
      "logistic regression             670/799    4.814e-01  6.089e-02  1.800e-01 \n",
      "logistic regression             680/799    4.811e-01  6.162e-02  1.835e-01 \n",
      "logistic regression             690/799    4.807e-01  6.234e-02  1.869e-01 \n",
      "logistic regression             700/799    4.804e-01  6.306e-02  1.903e-01 \n",
      "logistic regression             710/799    4.800e-01  6.377e-02  1.937e-01 \n",
      "logistic regression             720/799    4.797e-01  6.449e-02  1.972e-01 \n",
      "logistic regression             730/799    4.793e-01  6.520e-02  2.005e-01 \n",
      "logistic regression             740/799    4.790e-01  6.590e-02  2.039e-01 \n",
      "logistic regression             750/799    4.787e-01  6.660e-02  2.073e-01 \n",
      "logistic regression             760/799    4.784e-01  6.730e-02  2.106e-01 \n",
      "logistic regression             770/799    4.781e-01  6.800e-02  2.140e-01 \n",
      "logistic regression             780/799    4.778e-01  6.869e-02  2.173e-01 \n",
      "logistic regression             790/799    4.775e-01  6.938e-02  2.206e-01 \n",
      "logistic regression             799/799    4.772e-01  7.000e-02  2.236e-01 \n",
      "============================================================\n",
      "                    Logistic regression                     \n",
      "============================================================\n",
      "average loss         ...............................4.765e-01\n",
      "average accuracy     ...............................7.659e-01\n",
      "max accuracy         ...............................7.684e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "y_log = np.copy(y)\n",
    "y_log[np.where(y_log[:]== -1)] = 0\n",
    "\n",
    "max_iters = 800\n",
    "gamma = 1e-6\n",
    "# Logistic regression\n",
    "scores, weights,loss_avg , score_avg = cross_validation(y_log, X_sigmoid, 'LR', \n",
    "                                                        {\n",
    "                                                            \"initial_w\": initial_w,\n",
    "                                                            \"gamma\": gamma,\n",
    "                                                            \"max_iters\": max_iters,\n",
    "                                                        },\n",
    "                                                        threshold = 0.5,\n",
    "                                                         k=2, seed=0)\n",
    "print_score(loss_avg, score_avg, scores, method='Logistic regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "regularized logistic regression    0/499    6.491e-01  -1.449e-02 -1.144e-02\n",
      "regularized logistic regression   10/499    6.077e-01  -5.405e-03 -1.973e-02\n",
      "regularized logistic regression   20/499    5.909e-01  -2.400e-03 -2.751e-02\n",
      "regularized logistic regression   30/499    5.787e-01  1.176e-04  -3.262e-02\n",
      "regularized logistic regression   40/499    5.690e-01  2.314e-03  -3.583e-02\n",
      "regularized logistic regression   50/499    5.611e-01  4.280e-03  -3.764e-02\n",
      "regularized logistic regression   60/499    5.544e-01  6.072e-03  -3.837e-02\n",
      "regularized logistic regression   70/499    5.486e-01  7.727e-03  -3.825e-02\n",
      "regularized logistic regression   80/499    5.436e-01  9.271e-03  -3.745e-02\n",
      "regularized logistic regression   90/499    5.393e-01  1.072e-02  -3.610e-02\n",
      "regularized logistic regression  100/499    5.355e-01  1.210e-02  -3.430e-02\n",
      "regularized logistic regression  110/499    5.320e-01  1.342e-02  -3.213e-02\n",
      "regularized logistic regression  120/499    5.290e-01  1.468e-02  -2.964e-02\n",
      "regularized logistic regression  130/499    5.262e-01  1.589e-02  -2.690e-02\n",
      "regularized logistic regression  140/499    5.237e-01  1.706e-02  -2.394e-02\n",
      "regularized logistic regression  150/499    5.213e-01  1.820e-02  -2.080e-02\n",
      "regularized logistic regression  160/499    5.192e-01  1.931e-02  -1.751e-02\n",
      "regularized logistic regression  170/499    5.173e-01  2.038e-02  -1.408e-02\n",
      "regularized logistic regression  180/499    5.154e-01  2.144e-02  -1.055e-02\n",
      "regularized logistic regression  190/499    5.137e-01  2.248e-02  -6.927e-03\n",
      "regularized logistic regression  200/499    5.121e-01  2.349e-02  -3.226e-03\n",
      "regularized logistic regression  210/499    5.107e-01  2.449e-02  5.394e-04 \n",
      "regularized logistic regression  220/499    5.092e-01  2.547e-02  4.358e-03 \n",
      "regularized logistic regression  230/499    5.079e-01  2.645e-02  8.220e-03 \n",
      "regularized logistic regression  240/499    5.067e-01  2.740e-02  1.212e-02 \n",
      "regularized logistic regression  250/499    5.055e-01  2.835e-02  1.605e-02 \n",
      "regularized logistic regression  260/499    5.043e-01  2.929e-02  2.000e-02 \n",
      "regularized logistic regression  270/499    5.032e-01  3.021e-02  2.396e-02 \n",
      "regularized logistic regression  280/499    5.022e-01  3.113e-02  2.794e-02 \n",
      "regularized logistic regression  290/499    5.012e-01  3.204e-02  3.193e-02 \n",
      "regularized logistic regression  300/499    5.003e-01  3.294e-02  3.593e-02 \n",
      "regularized logistic regression  310/499    4.994e-01  3.384e-02  3.992e-02 \n",
      "regularized logistic regression  320/499    4.985e-01  3.472e-02  4.392e-02 \n",
      "regularized logistic regression  330/499    4.976e-01  3.561e-02  4.791e-02 \n",
      "regularized logistic regression  340/499    4.968e-01  3.648e-02  5.190e-02 \n",
      "regularized logistic regression  350/499    4.960e-01  3.735e-02  5.588e-02 \n",
      "regularized logistic regression  360/499    4.953e-01  3.822e-02  5.985e-02 \n",
      "regularized logistic regression  370/499    4.945e-01  3.908e-02  6.381e-02 \n",
      "regularized logistic regression  380/499    4.938e-01  3.993e-02  6.776e-02 \n",
      "regularized logistic regression  390/499    4.931e-01  4.078e-02  7.170e-02 \n",
      "regularized logistic regression  400/499    4.925e-01  4.163e-02  7.562e-02 \n",
      "regularized logistic regression  410/499    4.918e-01  4.247e-02  7.953e-02 \n",
      "regularized logistic regression  420/499    4.912e-01  4.330e-02  8.343e-02 \n",
      "regularized logistic regression  430/499    4.906e-01  4.414e-02  8.731e-02 \n",
      "regularized logistic regression  440/499    4.900e-01  4.497e-02  9.118e-02 \n",
      "regularized logistic regression  450/499    4.894e-01  4.579e-02  9.503e-02 \n",
      "regularized logistic regression  460/499    4.889e-01  4.661e-02  9.886e-02 \n",
      "regularized logistic regression  470/499    4.883e-01  4.743e-02  1.027e-01 \n",
      "regularized logistic regression  480/499    4.878e-01  4.824e-02  1.065e-01 \n",
      "regularized logistic regression  490/499    4.873e-01  4.905e-02  1.103e-01 \n",
      "regularized logistic regression  499/499    4.868e-01  4.977e-02  1.136e-01 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "method                            step         loss        w0         w1    \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "regularized logistic regression    0/499    6.504e-01  -1.427e-02 -1.129e-02\n",
      "regularized logistic regression   10/499    6.093e-01  -5.329e-03 -1.954e-02\n",
      "regularized logistic regression   20/499    5.927e-01  -2.399e-03 -2.717e-02\n",
      "regularized logistic regression   30/499    5.806e-01  4.545e-05  -3.211e-02\n",
      "regularized logistic regression   40/499    5.711e-01  2.173e-03  -3.515e-02\n",
      "regularized logistic regression   50/499    5.632e-01  4.073e-03  -3.677e-02\n",
      "regularized logistic regression   60/499    5.566e-01  5.801e-03  -3.733e-02\n",
      "regularized logistic regression   70/499    5.509e-01  7.395e-03  -3.703e-02\n",
      "regularized logistic regression   80/499    5.460e-01  8.880e-03  -3.607e-02\n",
      "regularized logistic regression   90/499    5.417e-01  1.028e-02  -3.455e-02\n",
      "regularized logistic regression  100/499    5.379e-01  1.160e-02  -3.259e-02\n",
      "regularized logistic regression  110/499    5.345e-01  1.286e-02  -3.026e-02\n",
      "regularized logistic regression  120/499    5.314e-01  1.407e-02  -2.762e-02\n",
      "regularized logistic regression  130/499    5.286e-01  1.523e-02  -2.474e-02\n",
      "regularized logistic regression  140/499    5.261e-01  1.635e-02  -2.164e-02\n",
      "regularized logistic regression  150/499    5.238e-01  1.744e-02  -1.836e-02\n",
      "regularized logistic regression  160/499    5.217e-01  1.850e-02  -1.493e-02\n",
      "regularized logistic regression  170/499    5.197e-01  1.954e-02  -1.138e-02\n",
      "regularized logistic regression  180/499    5.179e-01  2.055e-02  -7.730e-03\n",
      "regularized logistic regression  190/499    5.162e-01  2.154e-02  -3.990e-03\n",
      "regularized logistic regression  200/499    5.146e-01  2.252e-02  -1.781e-04\n",
      "regularized logistic regression  210/499    5.131e-01  2.348e-02  3.693e-03 \n",
      "regularized logistic regression  220/499    5.117e-01  2.442e-02  7.612e-03 \n",
      "regularized logistic regression  230/499    5.103e-01  2.536e-02  1.157e-02 \n",
      "regularized logistic regression  240/499    5.091e-01  2.628e-02  1.556e-02 \n",
      "regularized logistic regression  250/499    5.078e-01  2.719e-02  1.957e-02 \n",
      "regularized logistic regression  260/499    5.067e-01  2.809e-02  2.361e-02 \n",
      "regularized logistic regression  270/499    5.056e-01  2.898e-02  2.765e-02 \n",
      "regularized logistic regression  280/499    5.045e-01  2.987e-02  3.170e-02 \n",
      "regularized logistic regression  290/499    5.035e-01  3.074e-02  3.576e-02 \n",
      "regularized logistic regression  300/499    5.026e-01  3.161e-02  3.982e-02 \n",
      "regularized logistic regression  310/499    5.017e-01  3.248e-02  4.388e-02 \n",
      "regularized logistic regression  320/499    5.008e-01  3.334e-02  4.793e-02 \n",
      "regularized logistic regression  330/499    4.999e-01  3.419e-02  5.198e-02 \n",
      "regularized logistic regression  340/499    4.991e-01  3.504e-02  5.601e-02 \n",
      "regularized logistic regression  350/499    4.983e-01  3.588e-02  6.004e-02 \n",
      "regularized logistic regression  360/499    4.975e-01  3.672e-02  6.405e-02 \n",
      "regularized logistic regression  370/499    4.967e-01  3.755e-02  6.806e-02 \n",
      "regularized logistic regression  380/499    4.960e-01  3.838e-02  7.204e-02 \n",
      "regularized logistic regression  390/499    4.953e-01  3.921e-02  7.602e-02 \n",
      "regularized logistic regression  400/499    4.946e-01  4.003e-02  7.997e-02 \n",
      "regularized logistic regression  410/499    4.940e-01  4.085e-02  8.391e-02 \n",
      "regularized logistic regression  420/499    4.933e-01  4.166e-02  8.783e-02 \n",
      "regularized logistic regression  430/499    4.927e-01  4.247e-02  9.174e-02 \n",
      "regularized logistic regression  440/499    4.921e-01  4.328e-02  9.563e-02 \n",
      "regularized logistic regression  450/499    4.915e-01  4.408e-02  9.949e-02 \n",
      "regularized logistic regression  460/499    4.909e-01  4.488e-02  1.033e-01 \n",
      "regularized logistic regression  470/499    4.904e-01  4.568e-02  1.072e-01 \n",
      "regularized logistic regression  480/499    4.898e-01  4.647e-02  1.110e-01 \n",
      "regularized logistic regression  490/499    4.893e-01  4.726e-02  1.148e-01 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularized logistic regression  499/499    4.888e-01  4.797e-02  1.182e-01 \n",
      "============================================================\n",
      "              Regularized logistic regression               \n",
      "============================================================\n",
      "average loss         ...............................4.879e-01\n",
      "average accuracy     ...............................7.372e-01\n",
      "max accuracy         ...............................7.392e-01\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "y_log = np.copy(y)\n",
    "y_log[np.where(y_log[:]== -1)] = 0\n",
    "\n",
    "max_iters = 500\n",
    "gamma = 1e-6\n",
    "lambda_ = 1e-5\n",
    "\n",
    "# Regularized logistic regression\n",
    "scores, weights,loss_avg , score_avg = cross_validation(y_log, X_sigmoid, 'RLR', \n",
    "                                                        {\n",
    "                                                            \"initial_w\": initial_w,\n",
    "                                                            \"gamma\": gamma,\n",
    "                                                            \"max_iters\": max_iters,\n",
    "                                                            \"lambda_\": lambda_\n",
    "                                                        },\n",
    "                                                        threshold = 0.6,\n",
    "                                                         k=2, seed=0)\n",
    "print_score(loss_avg, score_avg, scores, method='Regularized logistic regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
